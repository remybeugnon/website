head(d, 10)
col.pal = colorRampPalette(colors = c('brown','Darkgreen'))
wordcloud2(data = d,
size = 1,
color = col.pal(10))
scholar.id2 = "OV0sKRkAAAAJ"
pub2 = get_publications(scholar.id2)
blogdown:::preview_site()
scholar.id2 = "OV0sKRkAAAAJ"
pub2 = get_publications(scholar.id2)
libs <- c(
'dplyr',
'ggplot2',
'scholar',
'easyPubMed'
)
invisible(lapply(libs, library, character.only = T))
scholar.id = "fEd4WGYAAAAJ"
pub = get_publications(scholar.id)
pub
title = pub$title[1]
title
my_request <- get_pubmed_ids_by_fulltitle(title, field = "[Title]")
my_request
my_xml <- fetch_pubmed_data(my_request)
my_xml
custom_grep(my_xml, tag = "Affiliation") %>% unlist
load("affiliation.RData")
affiliations
library(maps)
head(world.cities)
# New dataset
df.cities = data.frame(matrix(NA, ncol = 5, nrow = 1))
colnames(df.cities) = c('affiliation', 'City', 'Country', 'lat', 'long')
for(af in unique(affiliations$affiliation)){
city = world.cities$name[lapply(world.cities$name, function(x) {
grepl(pattern = x,
x = af,
fixed = T)}) %>% unlist]
country = world.cities$country.etc[lapply(world.cities$country.etc, function(x) {
grepl(pattern = x,
x = af,
fixed = T)}) %>% unlist]
c =
world.cities %>%
mutate(affiliation = af) %>%
filter(name %in% city & country.etc %in% country) %>%
select(affiliation, City = name, Country = country.etc, lat, long) %>%
filter(!duplicated(City)) # To remove multiple entries
df.cities = bind_rows(
df.cities,
c
)
}
affiliations = left_join(affiliations, df.cities, by = "affiliation")
affiliations
pub %>%
data.frame() %>%
select(Title = title, Authors = author,
Journal = journal, `Vol(Issue),page` = number,
Year = year) %>%
kableExtra::kable()
library("tm")
library("SnowballC")
library("wordcloud2")
# List of words to remove
list.stop = c('affect', 'effects', 'using')
docs <- Corpus(VectorSource(pub$title))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- docs %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., content_transformer(tolower)) %>%
tm_map(., removeNumbers) %>%
tm_map(., removeWords, stopwords("english")) %>%
tm_map(., removeWords, list.stop) %>%
tm_map(., removePunctuation) %>%
tm_map(., stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
col.pal = colorRampPalette(colors = c('brown','Darkgreen'))
wordcloud2(data = d,
size = 1,
color = col.pal(10))
scholar.id2 = "OV0sKRkAAAAJ"
pub2 = get_publications(scholar.id2)
docs <- Corpus(VectorSource(pub2$title))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- docs %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., content_transformer(tolower)) %>%
tm_map(., removeNumbers) %>%
tm_map(., removeWords, stopwords("english")) %>%
tm_map(., removeWords, list.stop) %>%
tm_map(., removePunctuation) %>%
tm_map(., stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
col.pal = colorRampPalette(colors = c('brown','Darkgreen'))
wordcloud2(data = d,
size = 1,
color = col.pal(10))
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
libs <- c(
'dplyr',
'ggplot2',
'scholar',
'easyPubMed'
)
invisible(lapply(libs, library, character.only = T))
scholar.id = "fEd4WGYAAAAJ"
pub = get_publications(scholar.id)
pub
title = pub$title[1]
title
my_request <- get_pubmed_ids_by_fulltitle(title, field = "[Title]")
my_request
my_xml <- fetch_pubmed_data(my_request)
my_xml
custom_grep(my_xml, tag = "Affiliation") %>% unlist
affiliations = data.frame(matrix(NA, ncol = 2, nrow = 1))
colnames(affiliations) = c('title', 'affiliation')
for(j in 1 : length(pub$title)){
skip_to_next <- FALSE
tryCatch( # I use tryCatch to catch the papers not found on PubMed
{
my_entrez_id <- get_pubmed_ids_by_fulltitle(pub$title[j], field = "[Title]")
my_xml <- fetch_pubmed_data(my_entrez_id)
affiliations = bind_rows(
affiliations,
data.frame(
title = pub$title[j],
affiliation = custom_grep(my_xml, tag = "Affiliation") %>% unlist
)
)
}, error = function(e){
skip_to_next <<- TRUE
}
)
if(skip_to_next) {
affiliations = bind_rows(
affiliations,
data.frame(
title = pub$title[j],
affiliation = "NOT FOUND")
)
next }  # To continue the script in the next occurrence
}
affiliations
save(affiliations, file = "affiliation.RData")
load("affiliation.RData")
affiliations
blogdown:::preview_site()
blogdown:::preview_site()
form = read_sheet("https://docs.google.com/spreadsheets/d/1n3tFeJqkQ1ppG3no9NSYtIg2v_1Ml4eTgYki9hHXCcA/edit?usp=sharing")
library(googlesheets4)
library(googlesheets4)
library(dplyr)
library(kableExtra)
library(googlesheets4)
library(dplyr)
library(kableExtra)
form = read_sheet("https://docs.google.com/spreadsheets/d/1n3tFeJqkQ1ppG3no9NSYtIg2v_1Ml4eTgYki9hHXCcA/edit?usp=sharing")
form
form %>% colnames %>% t() %>% kable
form %>% colnames %>% kable
head(R1)
R1 = form$`What 3 words would you use to describe our lab culture?`
head(R1)
library(SnowballC)
library(tm)
TextDoc <- Corpus(VectorSource(R1))
TextDoc
TextDoc =  TextDoc %>%
tm_map(TextDoc, toSpace, "/") %>%
tm_map(TextDoc, toSpace, "@") %>%
tm_map(TextDoc, toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(TextDoc, content_transformer(tolower)) %>%
# Remove numbers
tm_map(TextDoc, removeNumbers) %>%
# Remove english common stopwords
tm_map(TextDoc, removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(TextDoc, removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(TextDoc, removePunctuation) %>%
# Eliminate extra white spaces
tm_map(TextDoc, stripWhitespace) %>%
# Text stemming - which reduces words to their root form
tm_map(TextDoc, stemDocument)
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(TextDoc, toSpace, "/") %>%
tm_map(TextDoc, toSpace, "@") %>%
tm_map(TextDoc, toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(TextDoc, content_transformer(tolower)) %>%
# Remove numbers
tm_map(TextDoc, removeNumbers) %>%
# Remove english common stopwords
tm_map(TextDoc, removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(TextDoc, removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(TextDoc, removePunctuation) %>%
# Eliminate extra white spaces
tm_map(TextDoc, stripWhitespace) %>%
# Text stemming - which reduces words to their root form
tm_map(TextDoc, stemDocument)
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(TextDoc, toSpace, "/") %>%
tm_map(TextDoc, toSpace, "@") %>%
tm_map(TextDoc, toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(TextDoc, content_transformer(tolower)) %>%
# Remove numbers
tm_map(TextDoc, removeNumbers) %>%
# Remove english common stopwords
tm_map(TextDoc, removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(TextDoc, removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(TextDoc, removePunctuation) %>%
# Eliminate extra white spaces
tm_map(TextDoc, stripWhitespace) %>%
# Text stemming - which reduces words to their root form
tm_map(TextDoc, stemDocument)
TextDoc
TextDoc =  TextDoc %>%
tm_map(TextDoc, toSpace, "/") %>%
tm_map(TextDoc, toSpace, "@") %>%
tm_map(TextDoc, toSpace, "\\|")
TextDoc =  TextDoc %>%
tm_map(TextDoc, toSpace, "/")
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace) %>%
# Text stemming - which reduces words to their root form
tm_map(., stemDocument)
TextDoc
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
wordcloud2(dtm_d)
library(wordcloud2)
wordcloud2(dtm_d)
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
library(wordcloud2)
wordcloud2(dtm_d)
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation() ) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation() ) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Text stemming - which reduces words to their root form
#tm_map(., stemDocument)
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
library(wordcloud2)
wordcloud2(dtm_d)
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
o
wordcloud2(dtm_d)
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
# tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
# tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
# tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
# tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
wordcloud2(dtm_d)
dtm_d
blogdown::serve_site()
library(googlesheets4)
library(dplyr)
library(kableExtra)
form = read_sheet("https://docs.google.com/spreadsheets/d/1n3tFeJqkQ1ppG3no9NSYtIg2v_1Ml4eTgYki9hHXCcA/edit?usp=sharing")
head(form)
form %>% colnames %>% kable
R1 = form$`What 3 words would you use to describe our lab culture?`
head(R1)
library(SnowballC)
library(tm)
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Text stemming - which reduces words to their root form
# tm_map(., stemDocument)
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
library(wordcloud2)
wordcloud2(dtm_d)
library(wordcloud2)
wordcloud2(dtm_d)
library(wordcloud)
wordcloud(dtm_d)
wordcloud(words = dtm_d$word)
wordcloud(words = dtm_d$word, freq = dtm_d$freq)
