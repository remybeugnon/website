docs <- docs %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., content_transformer(tolower)) %>%
tm_map(., removeNumbers) %>%
tm_map(., removeWords, stopwords("english")) %>%
tm_map(., removeWords, list.stop) %>%
tm_map(., removePunctuation) %>%
tm_map(., stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
col.pal = colorRampPalette(colors = c('brown','Darkgreen'))
wordcloud2(data = d,
size = 1,
color = col.pal(10))
scholar.id2 = "OV0sKRkAAAAJ"
pub2 = get_publications(scholar.id2)
docs <- Corpus(VectorSource(pub2$title))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- docs %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., content_transformer(tolower)) %>%
tm_map(., removeNumbers) %>%
tm_map(., removeWords, stopwords("english")) %>%
tm_map(., removeWords, list.stop) %>%
tm_map(., removePunctuation) %>%
tm_map(., stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
col.pal = colorRampPalette(colors = c('brown','Darkgreen'))
wordcloud2(data = d,
size = 1,
color = col.pal(10))
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
libs <- c(
'dplyr',
'ggplot2',
'scholar',
'easyPubMed'
)
invisible(lapply(libs, library, character.only = T))
scholar.id = "fEd4WGYAAAAJ"
pub = get_publications(scholar.id)
pub
title = pub$title[1]
title
my_request <- get_pubmed_ids_by_fulltitle(title, field = "[Title]")
my_request
my_xml <- fetch_pubmed_data(my_request)
my_xml
custom_grep(my_xml, tag = "Affiliation") %>% unlist
affiliations = data.frame(matrix(NA, ncol = 2, nrow = 1))
colnames(affiliations) = c('title', 'affiliation')
for(j in 1 : length(pub$title)){
skip_to_next <- FALSE
tryCatch( # I use tryCatch to catch the papers not found on PubMed
{
my_entrez_id <- get_pubmed_ids_by_fulltitle(pub$title[j], field = "[Title]")
my_xml <- fetch_pubmed_data(my_entrez_id)
affiliations = bind_rows(
affiliations,
data.frame(
title = pub$title[j],
affiliation = custom_grep(my_xml, tag = "Affiliation") %>% unlist
)
)
}, error = function(e){
skip_to_next <<- TRUE
}
)
if(skip_to_next) {
affiliations = bind_rows(
affiliations,
data.frame(
title = pub$title[j],
affiliation = "NOT FOUND")
)
next }  # To continue the script in the next occurrence
}
affiliations
save(affiliations, file = "affiliation.RData")
load("affiliation.RData")
affiliations
blogdown:::preview_site()
blogdown:::preview_site()
form = read_sheet("https://docs.google.com/spreadsheets/d/1n3tFeJqkQ1ppG3no9NSYtIg2v_1Ml4eTgYki9hHXCcA/edit?usp=sharing")
library(googlesheets4)
library(googlesheets4)
library(dplyr)
library(kableExtra)
library(googlesheets4)
library(dplyr)
library(kableExtra)
form = read_sheet("https://docs.google.com/spreadsheets/d/1n3tFeJqkQ1ppG3no9NSYtIg2v_1Ml4eTgYki9hHXCcA/edit?usp=sharing")
form
form %>% colnames %>% t() %>% kable
form %>% colnames %>% kable
head(R1)
R1 = form$`What 3 words would you use to describe our lab culture?`
head(R1)
library(SnowballC)
library(tm)
TextDoc <- Corpus(VectorSource(R1))
TextDoc
TextDoc =  TextDoc %>%
tm_map(TextDoc, toSpace, "/") %>%
tm_map(TextDoc, toSpace, "@") %>%
tm_map(TextDoc, toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(TextDoc, content_transformer(tolower)) %>%
# Remove numbers
tm_map(TextDoc, removeNumbers) %>%
# Remove english common stopwords
tm_map(TextDoc, removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(TextDoc, removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(TextDoc, removePunctuation) %>%
# Eliminate extra white spaces
tm_map(TextDoc, stripWhitespace) %>%
# Text stemming - which reduces words to their root form
tm_map(TextDoc, stemDocument)
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(TextDoc, toSpace, "/") %>%
tm_map(TextDoc, toSpace, "@") %>%
tm_map(TextDoc, toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(TextDoc, content_transformer(tolower)) %>%
# Remove numbers
tm_map(TextDoc, removeNumbers) %>%
# Remove english common stopwords
tm_map(TextDoc, removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(TextDoc, removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(TextDoc, removePunctuation) %>%
# Eliminate extra white spaces
tm_map(TextDoc, stripWhitespace) %>%
# Text stemming - which reduces words to their root form
tm_map(TextDoc, stemDocument)
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(TextDoc, toSpace, "/") %>%
tm_map(TextDoc, toSpace, "@") %>%
tm_map(TextDoc, toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(TextDoc, content_transformer(tolower)) %>%
# Remove numbers
tm_map(TextDoc, removeNumbers) %>%
# Remove english common stopwords
tm_map(TextDoc, removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(TextDoc, removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(TextDoc, removePunctuation) %>%
# Eliminate extra white spaces
tm_map(TextDoc, stripWhitespace) %>%
# Text stemming - which reduces words to their root form
tm_map(TextDoc, stemDocument)
TextDoc
TextDoc =  TextDoc %>%
tm_map(TextDoc, toSpace, "/") %>%
tm_map(TextDoc, toSpace, "@") %>%
tm_map(TextDoc, toSpace, "\\|")
TextDoc =  TextDoc %>%
tm_map(TextDoc, toSpace, "/")
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace) %>%
# Text stemming - which reduces words to their root form
tm_map(., stemDocument)
TextDoc
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
wordcloud2(dtm_d)
library(wordcloud2)
wordcloud2(dtm_d)
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
library(wordcloud2)
wordcloud2(dtm_d)
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation() ) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation() ) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Text stemming - which reduces words to their root form
#tm_map(., stemDocument)
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
library(wordcloud2)
wordcloud2(dtm_d)
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
o
wordcloud2(dtm_d)
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
# tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
# tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
# tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
# tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
wordcloud2(dtm_d)
dtm_d
blogdown::serve_site()
library(googlesheets4)
library(dplyr)
library(kableExtra)
form = read_sheet("https://docs.google.com/spreadsheets/d/1n3tFeJqkQ1ppG3no9NSYtIg2v_1Ml4eTgYki9hHXCcA/edit?usp=sharing")
head(form)
form %>% colnames %>% kable
R1 = form$`What 3 words would you use to describe our lab culture?`
head(R1)
library(SnowballC)
library(tm)
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Text stemming - which reduces words to their root form
# tm_map(., stemDocument)
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
library(wordcloud2)
wordcloud2(dtm_d)
library(wordcloud2)
wordcloud2(dtm_d)
library(wordcloud)
wordcloud(dtm_d)
wordcloud(words = dtm_d$word)
wordcloud(words = dtm_d$word, freq = dtm_d$freq)
library(googlesheets4)
library(dplyr)
library(kableExtra)
library(lubridate)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form
format(Sys.Date()-7
, "%Y-%m-%d")
library(googlesheets4)
library(dplyr)
library(kableExtra)
library(lubridate)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form =
form %>%
distinct(Title, .keep_all = TRUE) %>%
distinct(`DOI (full URL)`, .keep_all = TRUE) %>%
filter(!is.na(Title))
form %>% filter(Horodateur > (format(Sys.Date()-7, "%Y-%m-%d")) ) %>%
mutate(text=paste("<b>", Title, "</b>", " <a href=\"",`DOI (full URL)`,"\">(DOI) </a> ", if(!is.na(Keywords)){Keywords}, if(!is.na(`Comments:`)){`Comments:`},sep="")) %>%
select(text) %>%
kable(., format = "html", escape = FALSE, col.names = NULL) %>%
kable_styling(bootstrap_options = c("hover", "condensed"))
form %>% filter(Horodateur > (format(Sys.Date()-7, "%Y-%m-%d")) )
form =
form %>%
distinct(Title, .keep_all = TRUE) %>%
distinct(`DOI (full URL)`, .keep_all = TRUE) %>%
filter(!is.na(Title))
form
form =
form %>%
distinct(Title, .keep_all = TRUE)
form %>%
distinct(Title, .keep_all = TRUE)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form %>%
distinct(Title, .keep_all = TRUE)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form
form %>%
distinct(Title, .keep_all = TRUE)
form %>%
distinct(Title, .keep_all = TRUE)
form %>%
# distinct(Title, .keep_all = TRUE) %>%
distinct(`DOI (full URL)`, .keep_all = TRUE)
blogdown:::preview_site()
form = form[order(-from$Horodateur)]
form = form[order(- Horodateur)]
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form = form[order(- from$Horodateur)]
form = form[order(from$Horodateur)]
form = form[order(from$Horodateur),]
form
form = form[-order(from$Horodateur),]
form
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form = form[order(from$Horodateur,  decreasing = T),]
form
blogdown::serve_site()
