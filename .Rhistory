TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace) %>%
# Text stemming - which reduces words to their root form
tm_map(., stemDocument)
TextDoc
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
wordcloud2(dtm_d)
library(wordcloud2)
wordcloud2(dtm_d)
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
library(wordcloud2)
wordcloud2(dtm_d)
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation() ) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation() ) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Text stemming - which reduces words to their root form
#tm_map(., stemDocument)
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
library(wordcloud2)
wordcloud2(dtm_d)
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
o
wordcloud2(dtm_d)
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
# tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
# tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
# tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
# tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
# tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
wordcloud2(dtm_d)
dtm_d
blogdown::serve_site()
library(googlesheets4)
library(dplyr)
library(kableExtra)
form = read_sheet("https://docs.google.com/spreadsheets/d/1n3tFeJqkQ1ppG3no9NSYtIg2v_1Ml4eTgYki9hHXCcA/edit?usp=sharing")
head(form)
form %>% colnames %>% kable
R1 = form$`What 3 words would you use to describe our lab culture?`
head(R1)
library(SnowballC)
library(tm)
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Text stemming - which reduces words to their root form
# tm_map(., stemDocument)
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
library(wordcloud2)
wordcloud2(dtm_d)
library(wordcloud2)
wordcloud2(dtm_d)
library(wordcloud)
wordcloud(dtm_d)
wordcloud(words = dtm_d$word)
wordcloud(words = dtm_d$word, freq = dtm_d$freq)
library(googlesheets4)
library(dplyr)
library(kableExtra)
library(lubridate)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form
format(Sys.Date()-7
, "%Y-%m-%d")
library(googlesheets4)
library(dplyr)
library(kableExtra)
library(lubridate)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form =
form %>%
distinct(Title, .keep_all = TRUE) %>%
distinct(`DOI (full URL)`, .keep_all = TRUE) %>%
filter(!is.na(Title))
form %>% filter(Horodateur > (format(Sys.Date()-7, "%Y-%m-%d")) ) %>%
mutate(text=paste("<b>", Title, "</b>", " <a href=\"",`DOI (full URL)`,"\">(DOI) </a> ", if(!is.na(Keywords)){Keywords}, if(!is.na(`Comments:`)){`Comments:`},sep="")) %>%
select(text) %>%
kable(., format = "html", escape = FALSE, col.names = NULL) %>%
kable_styling(bootstrap_options = c("hover", "condensed"))
form %>% filter(Horodateur > (format(Sys.Date()-7, "%Y-%m-%d")) )
form =
form %>%
distinct(Title, .keep_all = TRUE) %>%
distinct(`DOI (full URL)`, .keep_all = TRUE) %>%
filter(!is.na(Title))
form
form =
form %>%
distinct(Title, .keep_all = TRUE)
form %>%
distinct(Title, .keep_all = TRUE)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form %>%
distinct(Title, .keep_all = TRUE)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form
form %>%
distinct(Title, .keep_all = TRUE)
form %>%
distinct(Title, .keep_all = TRUE)
form %>%
# distinct(Title, .keep_all = TRUE) %>%
distinct(`DOI (full URL)`, .keep_all = TRUE)
blogdown:::preview_site()
form = form[order(-from$Horodateur)]
form = form[order(- Horodateur)]
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form = form[order(- from$Horodateur)]
form = form[order(from$Horodateur)]
form = form[order(from$Horodateur),]
form
form = form[-order(from$Horodateur),]
form
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form = form[order(from$Horodateur,  decreasing = T),]
form
blogdown::serve_site()
blogdown:::preview_site()
library(googlesheets4)
library(googlesheets4)
library(dplyr)
library(kableExtra)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form = form[order(form$Horodateur,  decreasing = T),]
form =
form %>%
# distinct(Title, .keep_all = TRUE) %>%
distinct(`DOI (full URL)`, .keep_all = TRUE) %>%
filter(!is.na(Title))
library(googlesheets4)
library(dplyr)
library(kableExtra)
form = read_sheet("https://docs.google.com/spreadsheets/d/1n3tFeJqkQ1ppG3no9NSYtIg2v_1Ml4eTgYki9hHXCcA/edit?usp=sharing")
form = read_sheet("https://docs.google.com/spreadsheets/d/1n3tFeJqkQ1ppG3no9NSYtIg2v_1Ml4eTgYki9hHXCcA/edit?usp=sharing")
head(form)
form %>% colnames %>% kable
R1 = form$`What 3 words would you use to describe our lab culture?`
head(R1)
library(SnowballC)
library(tm)
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
R1 = form$`What 3 words would you use to describe our lab culture?`
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
blogdown::serve_site()
library(googlesheets4)
library(dplyr)
library(kableExtra)
library(lubridate)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form = form[order(form$Horodateur,  decreasing = T),]
form =
form %>%
# distinct(Title, .keep_all = TRUE) %>%
distinct(`DOI (full URL)`, .keep_all = TRUE) %>%
filter(!is.na(Title))
form %>% filter(Horodateur > (format(Sys.Date()-7, "%Y-%m-%d")) ) %>%
mutate(text=paste("<b>", Title, "</b>", " <a href=\"",`DOI (full URL)`,"\">(DOI) </a> ", if(!is.na(Keywords)){Keywords}, if(!is.na(`Comments:`)){`Comments:`},sep="")) %>%
select(text) %>%
kable(., format = "html", escape = FALSE, col.names = NULL) %>%
kable_styling(bootstrap_options = c("hover", "condensed"))
form %>% filter(Horodateur > (format(Sys.Date()-7, "%Y-%m-%d")) )
library(googlesheets4)
library(dplyr)
library(kableExtra)
form = read_sheet("https://docs.google.com/spreadsheets/d/1n3tFeJqkQ1ppG3no9NSYtIg2v_1Ml4eTgYki9hHXCcA/edit?usp=sharing")
head(form)
R1 = form$`What 3 words would you use to describe our lab culture?`
head(R1)
library(SnowballC)
library(tm)
TextDoc <- Corpus(VectorSource(R1))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
your.stop.words = c()
TextDoc =  TextDoc %>%
tm_map(., toSpace, "/") %>%
tm_map(., toSpace, "@") %>%
tm_map(., toSpace, "\\|") %>%
# tm_map(., toSpace, ",") %>%
# Convert the text to lower case
tm_map(., content_transformer(tolower)) %>%
# Remove numbers
tm_map(., removeNumbers) %>%
# Remove english common stopwords
tm_map(., removeWords, stopwords("english")) %>%
# Remove your own stop word
tm_map(., removeWords, your.stop.words) %>%
# Remove punctuations
tm_map(., removePunctuation) %>%
# Eliminate extra white spaces
tm_map(., stripWhitespace)# %>%
# Text stemming - which reduces words to their root form
# tm_map(., stemDocument)
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
library(wordcloud)
wordcloud(words = dtm_d$word, freq = dtm_d$freq)
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::serve_site()
library(googlesheets4)
library(dplyr)
library(kableExtra)
library(lubridate)
form = read_sheet("https://docs.google.com/spreadsheets/d/1RbL6vgRC_ZvTYL8Qh3uoAFBNeuach2gnLiNW92KxwAA/edit?usp=sharing")
form$Horodateur = form$Horodateur %>% stringr::str_sub(.,1,10) %>% as.Date(.,format = "%Y-%m-%d")
form = form[order(form$Horodateur,  decreasing = T),]
form =
form %>%
# distinct(Title, .keep_all = TRUE) %>%
distinct(`DOI (full URL)`, .keep_all = TRUE) %>%
filter(!is.na(Title))
form %>% filter(Horodateur > (format(Sys.Date()-7, "%Y-%m-%d")) ) %>%
mutate(text=paste("<b>", Title, "</b>", " <a href=\"",`DOI (full URL)`,"\">(DOI) </a> ", if(!is.na(Keywords)){Keywords}, if(!is.na(`Comments:`)){`Comments:`},sep="")) %>%
select(text) %>%
kable(., format = "html", escape = FALSE, col.names = NULL) %>%
kable_styling(bootstrap_options = c("hover", "condensed"))
if((format(Sys.Date()-7, "%Y-%m-%d"))>format(floor_date(as.Date(Sys.Date()), "month"), "%Y-%m-%d")){
form %>% filter(Horodateur < (format(Sys.Date()-7, "%Y-%m-%d")) &
Horodateur > format(floor_date(as.Date(Sys.Date()), "month"), "%Y-%m-%d")) %>%
mutate(text=paste("<b>", Title, "</b>", " <a href=\"",`DOI (full URL)`,"\">(DOI) </a> ", if(!is.na(Keywords)){Keywords}, if(!is.na(`Comments:`)){`Comments:`},sep="")) %>%
select(text) %>%
kable(., format = "html", escape = FALSE, col.names = NULL) %>%
kable_styling(bootstrap_options = c("hover", "condensed"))
}
form %>% filter(Horodateur < format(floor_date(as.Date(Sys.Date()), "month"), "%Y-%m-%d") &
Horodateur > format(floor_date(as.Date(Sys.Date()), "year"), "%Y-%m-%d")) %>%
mutate(text=paste("<b>", Title, "</b>", " <a href=\"",`DOI (full URL)`,"\">(DOI) </a> ", if(!is.na(Keywords)){Keywords}, if(!is.na(`Comments:`)){`Comments:`},sep="")) %>%
select(text) %>%
kable(., format = "html", escape = FALSE, col.names = NULL) %>%
kable_styling(bootstrap_options = c("hover", "condensed"))
blogdown:::preview_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
